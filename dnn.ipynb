{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 64\n",
    "DROPOUT_RATE = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0\n",
    "BATCH_SIZE = 10\n",
    "CRITERION = F.nll_loss\n",
    "EPOCHS = 3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.fc1 = nn.Linear(input_dim, HIDDEN_DIM)\n",
    "        self.fc2 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.fc3 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.fc4 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.fc5 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        self.fc6 = nn.Linear(HIDDEN_DIM, output_dim)\n",
    "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "        self.grayscale = transforms.Grayscale()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.shape[1] == 3:\n",
    "            x = self.grayscale(x)\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.dropout(F.relu(self.fc4(x)))\n",
    "        x = self.dropout(F.relu(self.fc5(x)))\n",
    "        x = F.log_softmax(self.fc6(x), dim=1)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"./347data\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"./347data\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "validation_set_size = int(len(train) * 0.1)\n",
    "training_set_size = len(train) - validation_set_size\n",
    "train_set, validation_set = torch.utils.data.random_split(train, [training_set_size, validation_set_size])\n",
    "train_set = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_set = torch.utils.data.DataLoader(validation_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_set = torch.utils.data.DataLoader(validation_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5400/5400 [00:20<00:00, 260.83it/s]\n",
      "  1%|          | 30/5400 [00:00<00:18, 294.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.20145754516124725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5400/5400 [00:18<00:00, 289.52it/s]\n",
      "  1%|          | 29/5400 [00:00<00:18, 287.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.0837847962975502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5400/5400 [00:21<00:00, 251.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.5969727635383606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MNIST_dnn = DNN(28 * 28, 10).to(device)\n",
    "optimizer = optim.Adam(MNIST_dnn.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in tqdm(train_set):\n",
    "        X, y = data\n",
    "        MNIST_dnn.zero_grad()\n",
    "        output = MNIST_dnn(X.to(device))\n",
    "        loss = CRITERION(output, y.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1} Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.96\n",
      "Validation F1 Score: 0.9598386143410804\n",
      "Validation AUC Score: 0.9776275372677483\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "true = []\n",
    "MNIST_dnn.eval()\n",
    "with torch.no_grad():\n",
    "    for data in validation_set:\n",
    "        X, y = data\n",
    "        for i in MNIST_dnn(X.to(device)):\n",
    "            output.append(torch.argmax(i).cpu())\n",
    "        for i in y:\n",
    "            true.append(i)\n",
    "MNIST_dnn.train()\n",
    "print(\"Validation Accuracy:\", metrics.accuracy_score(true, output))\n",
    "print(\"Validation F1 Score:\", metrics.f1_score(true, output, average=\"macro\"))\n",
    "true = np.eye(10)[true]\n",
    "output = np.eye(10)[output]\n",
    "print(\"Validation AUC Score:\", metrics.roc_auc_score(true, output, multi_class=\"ovo\", average=\"macro\"))\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train = datasets.CIFAR10(\"./347data\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.CIFAR10(\"./347data\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "validation_set_size = int(len(train) * 0.1)\n",
    "training_set_size = len(train) - validation_set_size\n",
    "train_set, validation_set = torch.utils.data.random_split(train, [training_set_size, validation_set_size])\n",
    "train_set = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_set = torch.utils.data.DataLoader(validation_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_set = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:19<00:00, 255.11it/s]\n",
      "  0%|          | 22/5000 [00:00<00:22, 217.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.153488874435425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:22<00:00, 223.76it/s]\n",
      "  0%|          | 20/5000 [00:00<00:25, 193.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.8779704570770264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:22<00:00, 226.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.2098474502563477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CIFAR10_dnn = DNN(32 * 32, 10).to(device)\n",
    "optimizer = optim.Adam(CIFAR10_dnn.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in tqdm(train_set):\n",
    "        X, y = data\n",
    "        CIFAR10_dnn.zero_grad()\n",
    "        output = CIFAR10_dnn(X.to(device))\n",
    "        loss = CRITERION(output, y.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1} Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.2254\n",
      "Validation F1 Score: 0.20074097433389385\n",
      "Validation AUC Score: 0.5690189593265054\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "true = []\n",
    "CIFAR10_dnn.eval()\n",
    "with torch.no_grad():\n",
    "    for data in validation_set:\n",
    "        X, y = data\n",
    "        for i in CIFAR10_dnn(X.to(device)):\n",
    "            output.append(torch.argmax(i).cpu())\n",
    "        for i in y:\n",
    "            true.append(i)\n",
    "CIFAR10_dnn.train()\n",
    "print(\"Validation Accuracy:\", metrics.accuracy_score(true, output))\n",
    "print(\"Validation F1 Score:\", metrics.f1_score(true, output, average=\"macro\"))\n",
    "true = np.eye(10)[true]\n",
    "output = np.eye(10)[output]\n",
    "print(\"Validation AUC Score:\", metrics.roc_auc_score(true, output, multi_class=\"ovo\", average=\"macro\"))\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420 46 51\n"
     ]
    }
   ],
   "source": [
    "iyer = np.loadtxt(open(\"347data/iyer.txt\", \"rb\"), delimiter=\"\\t\")\n",
    "features = iyer[:, 2:].astype(float)\n",
    "labels = iyer[:, 1].astype(int)\n",
    "\n",
    "data = [] #data0\n",
    "for i in range(features.shape[0]):\n",
    "    stack = np.array([])\n",
    "    stack = np.column_stack([np.roll(features[i,], j, axis=0) for j in range(features.shape[1])]).astype(np.float32)\n",
    "    data.append([stack, np.array(labels[i] + 1, dtype=int)])\n",
    "\n",
    "data = np.array(data)\n",
    "np.random.shuffle(data)\n",
    "X = torch.tensor(np.array([i[0] for i in data])).view(-1, 1, 12, 12)\n",
    "y = torch.tensor(np.array([i[1] for i in data]))\n",
    "\n",
    "test_set_size = int(X.shape[0] * 0.1)\n",
    "training_set_size = X.shape[0] - test_set_size\n",
    "validation_set_size = int(training_set_size * 0.1)\n",
    "training_set_size -= validation_set_size\n",
    "print(training_set_size, validation_set_size, test_set_size)\n",
    "train_X = X[:training_set_size]\n",
    "train_y = y[:training_set_size]\n",
    "validation_X = X[training_set_size:training_set_size + validation_set_size]\n",
    "validation_y = y[training_set_size:training_set_size + validation_set_size]\n",
    "test_X = X[training_set_size + validation_set_size:]\n",
    "test_y = y[training_set_size + validation_set_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 437.20it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 487.36it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 491.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.1298329830169678\n",
      "Epoch 2 Loss: 1.6145617961883545\n",
      "Epoch 3 Loss: 1.3383407592773438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Iyer_dnn = DNN(12 * 12, 12).to(device)\n",
    "optimizer = optim.Adam(Iyer_dnn.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0, training_set_size, BATCH_SIZE)):\n",
    "        batch_X = train_X[i:i+BATCH_SIZE]\n",
    "        batch_y = train_y[i:i + BATCH_SIZE]\n",
    "        \n",
    "        Iyer_dnn.zero_grad()\n",
    "        output = Iyer_dnn(batch_X.to(device))\n",
    "        loss = CRITERION(output, batch_y.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1} Loss: {loss.item()}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.41304347826086957\n",
      "Validation F1 Score: 0.11273532668881506\n",
      "Validation AUC Score: 0.7080428196572216\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "true = []\n",
    "Iyer_dnn.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(validation_set_size):\n",
    "        output.append(torch.argmax(Iyer_dnn(validation_X[i].view(-1, 1, 12, 12).to(device))).cpu())\n",
    "        true.append(validation_y[i])\n",
    "Iyer_dnn.train()\n",
    "print(\"Validation Accuracy:\", metrics.accuracy_score(true, output))\n",
    "print(\"Validation F1 Score:\", metrics.f1_score(true, output, average=\"macro\"))\n",
    "labels = [0 for _ in range(0, 12)]\n",
    "for i in true:\n",
    "    labels[i] += 1\n",
    "for i, l in enumerate(labels):\n",
    "    if l == 0:\n",
    "        true.append(i) \n",
    "        output.append(i)\n",
    "labels = [0 for _ in range(0, 12)]\n",
    "for i in output:\n",
    "    labels[i] += 1\n",
    "for i, l in enumerate(labels):\n",
    "    if l == 0:\n",
    "        true.append(i) \n",
    "        output.append(i)\n",
    "true = np.eye(12)[true]\n",
    "output = np.eye(12)[output]\n",
    "print(\"Validation AUC Score:\", metrics.roc_auc_score(true, output, multi_class=\"ovo\", average=\"macro\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
